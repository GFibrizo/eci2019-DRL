{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método de Montecarlo para _blackjack_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Juan Gómez Romero**](https://decsai.ugr.es/~jgomez)  \n",
    "Departamento de Ciencias de la Computación e Inteligencia Artificial  \n",
    "Universidad de Granada  \n",
    "This work is licensed under the [GNU General Public License v3.0](https://choosealicense.com/licenses/gpl-3.0/).\n",
    "\n",
    "---\n",
    "\n",
    "Ejemplo basado en:\n",
    "> R.S. Sutton, A.G. Barto (2018) Reinforcement Learning. Chapter 5.1: Monte Carlo Prediction\n",
    "\n",
    "_The object of the popular casino card game of blackjack is to obtain cards the sum of whose numerical values is as great as possible without exceeding 21. All face cards count as 10, and an ace can count either as 1 or as 11. We consider the version in which each player competes independently against the dealer. The game begins with two cards dealt to both dealer and player. One of the dealer’s cards is face up and the other is face down. If the player has 21 immediately (an ace and a 10-card), it is called a natural. He then wins unless the dealer also has a natural, in which case the game is a draw. If the player does not have a natural, then he can request additional cards, one by one (hits), until he either stops (sticks) or exceeds 21 (goes bust). If he goes bust, he loses; if he sticks, then it becomes the dealer’s turn. The dealer hits or sticks according to a fixed strategy without choice: he sticks on any sum of 17 or greater, and hits otherwise. If the dealer goes bust, then the player wins; otherwise, the outcome—win, lose, or draw—is determined by whose final sum is closer to 21._\n",
    "\n",
    "> Udacity (2019) Deep Reinforcement Learning Course. Available in [GitHub](https://github.com/udacity/deep-reinforcement-learning/tree/master/monte-carlo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorar entorno virtual\n",
    "\n",
    "En este ejercicio utilizamos el entorno virtual [Blackjack-v0](https://gym.openai.com/envs/Blackjack-v0/) de [OpenAI](https://openai.com). \n",
    "\n",
    "En primer lugar, vamos a explorar cómo funciona este entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear entorno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada estado es una tupla de 3 elementos:\n",
    "- la puntuación del jugador $\\in \\{0, 1, \\ldots, 31\\}$, \n",
    "- la carta boca arriba $\\in \\{0, 1, \\ldots, 31\\}$, \n",
    "- si el jugador tiene un as utilizable (`no` $=0$, `sí` $=1$)\n",
    "\n",
    "El agente puede realizar dos acciones:\n",
    "\n",
    "```\n",
    "    PLANTARSE = 0\n",
    "    PEDIR CARTA = 1\n",
    "```\n",
    "\n",
    "En [Blackjack-v0](https://gym.openai.com/envs/Blackjack-v0/) la recompensa es siempre $0$, excepto cuando se gana el juego ($+1$). No obstante, el rango definido de posibles recompensas es $[-\\inf, +\\inf]$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(env.reward_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las acciones más relevantes sobre un entorno son:\n",
    "- `env.reset()`: resetear entorno antes de comenzar un episodio\n",
    "- `env.action_space.sample()`: obtener una acción aleatoria de entre las posibles\n",
    "- `env.step(action)`: aplicar `action` en el entorno y obtener:\n",
    "    - estado siguiente: `state`\n",
    "    - recompensa obtenida: `reward`\n",
    "    - si se completó la tarea: `done`\n",
    "    - información adicional: `info`\n",
    "- `env.reward_range()`: rango de recompensas\n",
    "\n",
    "El siguiente código implementa un jugador aleatorio (juega 3 partidas --> 3 episodios)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(3):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        print(state)\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print('Fin del juego! Recompensa: ', reward)\n",
    "            print('Ganaste :)\\n') if reward > 0 else print('Perdiste :(\\n')\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como cabría esperar, el agente no es muy efectivo y pierde en la mayoría de las ocasiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método de Montecarlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición de la tabla `Q`\n",
    "Antes de implementar el algoritmo, nos fijamos en la estructura de datos que implementará la tabla `Q`. Utilizaremos un diccionario que asignará a un par _(estado, acción)_ un valor numérico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "state_example = env.observation_space.sample()\n",
    "action_example = env.action_space.sample()\n",
    "Q = { (state_example, action_example) : 10.0 }\n",
    "\n",
    "print(Q)\n",
    "print(Q[state_example, action_example])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para simplificar la gestión de `Q` se utiliza una estructura [`defaultdict`](https://docs.python.org/3/library/collections.html#collections.defaultdict). La siguiente instrucción crea la tabla `Q` con valor inicial `0` para todos los pares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = defaultdict(lambda: np.zeros(env.action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pueden añadir directamente valores a la tabla `Q`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Q[state_example][action_example])\n",
    "Q[state_example][action_example] = 10.0\n",
    "print(Q[state_example][action_example])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo\n",
    "A continuación se proporciona una implementación del algoritmo de Montecarlo para [Blackjack-v0](https://gym.openai.com/envs/Blackjack-v0/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def generate_episode_from_Q_epsilon(bj_env, Q, epsilon):\n",
    "    \"\"\"Generador de episodios:\n",
    "    Params\n",
    "    ======\n",
    "    bj_env: entorno Blackjack-v0\n",
    "    Q: tabla Q, definida como un diccionario Q[state][action] = reward\n",
    "    epsilon: valor para epsilon-greedy\"\"\"\n",
    "    \n",
    "    episode = []\n",
    "    state = bj_env.reset()\n",
    "    while True:\n",
    "        r = np.random.uniform(size=1)[0]\n",
    "        if r <= epsilon:\n",
    "            action = bj_env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q[state])                \n",
    "        \n",
    "        next_state, reward, done, info = bj_env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return episode\n",
    "\n",
    "def mc_prediction_q(env, num_episodes, generate_episode, gamma=1.0, epsilon=0.2):\n",
    "    \"\"\"Algoritmo de Montecarlo:\n",
    "    Params\n",
    "    ======\n",
    "    env: entorno Blackjack-v0\n",
    "    num_episodes: numero de episodios a generar\n",
    "    generate_episode: funcion para generar episodio siguiente\n",
    "    gamma: tasa de descuento\n",
    "    epsilon: valor para epsilon-greedy\"\"\"\n",
    "\n",
    "    # inicializar diccionarios (prediccion every-visit)\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))  # Q\n",
    "    N = defaultdict(lambda: np.zeros(env.action_space.n))  # numero de visitas a (estado, accion)\n",
    "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n)) # suma de recompensa en (estado, accion)\n",
    "    \n",
    "    # bucle de episodios\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        \n",
    "        # monitorizar progreso\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # generar episodio\n",
    "        episode = generate_episode(env, Q, epsilon)\n",
    "        \n",
    "        # actualizar tabla Q\n",
    "        # - obtener estados, acciones y recompensas del episodio\n",
    "        states, actions, rewards = zip(*episode)        \n",
    "        # - obtener gamma para aplicar descuentos\n",
    "        discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "        # - actualizar suma de recompensa, numero de visitas y Q para cada (estado, accion) del episodio        \n",
    "        for i, state in enumerate(states):\n",
    "            returns_sum[state][actions[i]] += sum(rewards[i:]*discounts[:-(1+i)])\n",
    "            N[state][actions[i]] += 1.0\n",
    "            Q[state][actions[i]] = returns_sum[state][actions[i]] / N[state][actions[i]]\n",
    "    \n",
    "    policy = dict((k,np.argmax(v)) for k, v in Q.items())\n",
    "            \n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para llamar al algoritmo, creamos el entorno y llamamos a la función `mc_prediction_q`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v0')\n",
    "\n",
    "# obtener estimacion de Q (funcion accion-valor)\n",
    "Q, policy = mc_prediction_q(env, 500000, generate_episode_from_Q_epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de resultados\n",
    "Podemos representar gráficamente el valor de cada estado _(puntuación, carta, as)_ según la estimación que hemos hecho de `Q`. \n",
    "\n",
    "Para visualizar los resultados es necesario disponer del fichero [_plot_\\__utils.py_](https://github.com/jgromero/eci2019-DRL/blob/master/Tema%203%20-%20Aprendizaje%20por%20Refuerzo/code/plot_utils.py). \n",
    "\n",
    "(En Google Colaboratory, usar en el panel izquierdo, pestaña _Archivos_ > _Subir_. El fichero estará disponible hasta que se cierre el entorno de ejecución.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import plot_blackjack_values, plot_blackjack_policy\n",
    "\n",
    "# obtener funcion estado-valor (para cada estado, el valor maximo de las acciones posibles)\n",
    "V_to_plot = dict((k, np.max(v)) for k, v in Q.items())\n",
    "\n",
    "# representar gráficamente la función v(s) = v(puntuacion, carta, as_disponible)\n",
    "plot_blackjack_values(V_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# represent gráficamente la política\n",
    "plot_blackjack_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### EJERCICIO\n",
    "\n",
    "¿Podrías extender este código para otro de los entornos **discretos** de OpenAI Gym? Por ejemplo, [CliffWalking-v0](https://github.com/openai/gym/blob/master/gym/envs/toy_text/cliffwalking.py).\n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
