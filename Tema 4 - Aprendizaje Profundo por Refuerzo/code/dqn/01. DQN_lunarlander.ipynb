{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning para _lunar lander_\n",
    "\n",
    "[**Juan Gómez Romero**](https://decsai.ugr.es/~jgomez)  \n",
    "Departamento de Ciencias de la Computación e Inteligencia Artificial  \n",
    "Universidad de Granada  \n",
    "This work is licensed under the [GNU General Public License v3.0](https://choosealicense.com/licenses/gpl-3.0/).\n",
    "\n",
    "---\n",
    "Ejemplo basado en:\n",
    "> Udacity (2019) Deep Reinforcement Learning Course. Disponible en [GitHub](https://github.com/udacity/deep-reinforcement-learning/tree/master/dqn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorar entorno virtual\n",
    "\n",
    "En este ejercicio utilizamos el entorno virtual [LunarLander-v2](https://gym.openai.com/envs/LunarLander-v2/) de [OpenAI](https://openai.com). \n",
    "\n",
    "![](lunarlander.gif)\n",
    "\n",
    "En primer lugar, vamos a explorar cómo funciona este entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear entorno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada estado es una tupla de 8 elementos. Los dos primeros valores corresponden a la posición del módulo.\n",
    "\n",
    "El agente puede realizar 4 acciones:\n",
    "\n",
    "```\n",
    "    NADA = 0\n",
    "    ACTIVAR MOTOR DE ORIENTACION IZQUIERDO = 1\n",
    "    ACTIVAR MOTOR PRINCIPAL = 2\n",
    "    ACTIVAR MOTOR DE ORIENTACION DERECHO = 3\n",
    "```\n",
    "\n",
    "El objetivo en [LunarLander-v2](https://gym.openai.com/envs/LunarLander-v2/) es conseguir aterrizar el módulo lunar en el espacio designado ($+200$). La puntuación que se obtiene por moverse desde la parte superior de la pantalla hasta la zona de aterrizaje está en torno a $[100, 140]$. Si el módulo se desvía de la zona de aterrizaje pierde puntuación. El episodio termina si el módulo se estrella ($-100$) o se detiene ($+100$). Cada para que contacte con el suelo aumenta la puntuación ($+10$). Activar el motor principal supone una penalización ($-0.3$ por frame). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(8,)\n",
      "Discrete(4)\n",
      "(-inf, inf)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(env.reward_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(8,)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementación de un agente con comportamiento sin entrenar utilizando la clase [`Agent`](dqn_agent.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn_agent import Agent\n",
    "\n",
    "agent = Agent(state_size=8, action_size=4, seed=0)\n",
    "\n",
    "# watch an untrained agent\n",
    "state = env.reset()\n",
    "for j in range(200):\n",
    "    action = agent.act(state)\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break \n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo\n",
    "A continuación se proporciona una implementación genérica del algoritmo Deep Q-Learning (DQN) y su aplicación a [LunarLander-v2](https://gym.openai.com/envs/LunarLander-v2/).\n",
    "\n",
    "Se considera que el entorno [LunarLander-v2](https://gym.openai.com/envs/LunarLander-v2/) está resuelto cuando se obtienen más de $200$ puntos de media durante 100 episodios consecutivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 100\tPuntuacion media (100 anteriores): -180.33\n",
      "Episodio 200\tPuntuacion media (100 anteriores): -110.62\n",
      "Episodio 300\tPuntuacion media (100 anteriores): -56.03\n",
      "Episodio 400\tPuntuacion media (100 anteriores): 31.83\n",
      "Episodio 500\tPuntuacion media (100 anteriores): 142.84\n",
      "Episodio 501\tPuntuacion media (ultimos 100): 144.79"
     ]
    }
   ],
   "source": [
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): numero maximo de episodios de entrenamiento (n_episodios)\n",
    "        max_t (int): numero maximo de pasos por episodio (n_entrenamiento)\n",
    "        eps_start (float): valor inicial de epsilon\n",
    "        eps_end (float): valor final de epsilon\n",
    "        eps_decay (float): factor de multiplicacion (por episodio) de epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # puntuaciones de cada episodio\n",
    "    scores_window = deque(maxlen=100)  # puntuaciones de los ultimos 100 episodios\n",
    "    eps = eps_start                    # inicializar epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            \n",
    "            # elegir accion At con politica e-greedy\n",
    "            action = agent.act(state, eps)\n",
    "            \n",
    "            # aplicar At y obtener Rt+1, St+1\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # almacenar <St, At, Rt+1, St+1>\n",
    "            agent.memory.add(state, action, reward, next_state, done)\n",
    "            \n",
    "            # train & update\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            \n",
    "            # avanzar estado\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            if done:\n",
    "                break \n",
    "\n",
    "        scores_window.append(score)       # guardar ultima puntuacion\n",
    "        scores.append(score)              # guardar ultima puntuacion\n",
    "        eps = max(eps_end, eps_decay*eps) # reducir epsilon\n",
    "        \n",
    "        print('\\rEpisodio {}\\tPuntuacion media (ultimos {:d}): {:.2f}'.format(i_episode, 100, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisodio {}\\tPuntuacion media ({:d} anteriores): {:.2f}'.format(i_episode, 100, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=200.0:\n",
    "            print('\\nProblema resuelto en {:d} episodios!\\tPuntuacion media (ultimos {:d}): {:.2f}'.format(i_episode-100, 100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth') # guardar pesos de agente entrenado\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Puntuacion')\n",
    "plt.xlabel('Episodio #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizar agente entrenado\n",
    "\n",
    "Podemos visualizar el comportamiento del agente entrenado cargando los pesos del fichero donde se han almacenado. (Solo en entorno local, no en Google Collaboratory.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar pesos del fichero `checkpoint.pth`\n",
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "for i in range(3):\n",
    "    state = env.reset()\n",
    "    for j in range(200):\n",
    "        action = agent.act(state)\n",
    "        env.render()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break \n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### EJERCICIO\n",
    "\n",
    "¿Podrías extender este código para otro de los entornos de OpenAI Gym? Por ejemplo, [CartPole-v0](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py).\n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
